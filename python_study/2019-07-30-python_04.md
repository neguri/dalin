---
layout: post
title: "python (multiprocessing)"
description: "python basic study"
date: 2019-07-30
tags: [python]
comments: false
share: false
---
### Multiprocessing

`CPython`에서는 `GIL`의 영향을 받아서 멀티코어에서는 제 성능을 발휘하기 어렵기 때문에 멀티코어 환경에서는 `threading`보다는 `multiprocessing`을 권함.

#### Multiprocessing 구현

> threading 방식과 유사한데, 

``` python
import os
import multiprocessing

def worker(count):
    print("name: %s, argument : %s" %  (multiprocessing.curren_process().name, count))
    print("parent pid:%s  pid:%s" % (os.getppid(), os.getpid()))
    print("")
    
def main():
    for i in range(5):
        p = multiprocessing.Process(target=worker, name="process %i" % i, args=(i,))
        p.start()
if __name__ == "__main__":
    main()
```

> 실행 결과든 다음과 같은데, parent pid와 child pid도 확인 할 수 있음.  

``` bash
name: process 0, argument : 0
parent pid:51  pid:57

name: process 1, argument : 1
parent pid:51  pid:58

name: process 2, argument : 2
parent pid:51  pid:59

name: process 3, argument : 3
parent pid:51  pid:60

name: process 4, argument : 4
parent pid:51  pid:61
```

> 클래스를 이용하여 구현하는 것은 아래와 같음  

``` python
# basic multiprocessing

import os
import multiprocessing

class Worker(multiprocessing.Process):

  def __init__(self, name, args):
    multiprocessing.Process.__init__(self)
    self.name = name
    self.args = args
  
  def run(self):
    print("name: %s, argument: %s " % (self.name, self.args[0]))
    print("parent pid: %s, pid: %s" %(os.getppid(), os.getpid()))

def main():
  for i in range(5):
    p = Worker(name="process %i" % args(i,))
    p.start()


if __name__ == "__main__":
  main()
  ```

  > 실행결과는 아래와 같은데, 클래스를 사용하지 않는 것과 큰 차이는 없음. 다만 주의할 것은 child process를 띄울 때 재귀적으로 반복 실행 될 수 있으므로 &#95;&#95;__main__&#95;&#95;을 확인하는 부분이 들어가야 함.

``` bash
parent pid:610  pid:615
name: process 1, argument : 1

parent pid:610  pid:617

name: process 2, argument : 2
parent pid:610  pid:618

name: process 4, argument : 4
name: process 3, argument : 3

parent pid:610  pid:619
parent pid:610  pid:620
```

#### Multiprocessing의 로깅

> thread와 다르게 하나의 프로세스에서 로그를 남기는 것이 아니기 때문에 `print`를 사용하더라도 로그가 깨지는 경우는 없음.

``` python
import logging
import multiprocessing

def worker(count):
  print("count: %s" % count)

def main():
  multiprocessing.log_to_stderr()
  logger = multiprocessing.get_logger()
  logger.setLevel(logging.DEBUG)

  for i in range(5):
    t = multiprocessing.Process(target=worker, args=(i,))
    t.start()

if __name__ == "__main__":
  main()
  ```

> 출력해 보면 `DEBUG` 레벨로 설정되어서 너무 많은 메시지가 나옴. 디버깅 하기는 편하겠지만..


### Daemon Process
  
> Process도 Thread와 마찬가지로 daemon으로 동작하게 할 수 있음. 만드는 방법은 Thread와 같이 daemon 속성을 True로 설정하면 됨.  

``` python
# daemon process

import time
import multiprocessing

def daemon():
    print("Start")
    time.sleep(5)
    print("Exit")

def main():
    d = multiprocessing.Process(name="daemon", target=daemon)
    d.daemon = True
    
    d.start()
    time.sleep(3)
    print("Main process terminated")
    
if __name__ == "__main__":
    main()
```
> 실행하면 다음과 같이 main process가 종료된 이후에 daemon process가 종료됨  
``` bash
Start
Main process terminated
Exit
```
> 만약 daemon process 가 종료될 때 까지 기다리면 `join`을 사용하면 됨. 결과는 다음과 같음  
``` bash
Start
Exit
Main process terminated
```

### Process Exit  

> Thread의 경우 process 내에서 자식으로 띄운 thread를 종료할 방법이 없었는데, process의 경우 child process를 강제로 종료 할 수도 있고 상태의 확인 및 수행 결과를 받을 수도 있음  

``` python
# process exit

import sys
import time
import multiprocessing

def good_job():
    p = multiprocessing.current_process()
    print("Start name: %s, pid:%s" % (p.name, p.pid))
    time.sleep(5)
    print("Exit name:%s, pid:%s" % (p.name, p.pid))
    return 0

def fail_job():
    p = multiprocessing.current_process()
    print("Start name: %s, pid:%s" % (p.name, p.pid))
    time.sleep(5)
    print("Exit name:%s, pid:%s" % (p.name, p.pid))
    sys.exit(1)

def kill_job():
    p = multiprocessing.current_process()
    print("Start name: %s, pid:%s" % (p.name, p.pid))
    time.sleep(10)
    print("Exit name:%s, pid:%s" % (p.name, p.pid))
    return 0

def main():
    process_list = []
    for func in [good_job, fail_job, kill_job]:
        p = multiprocessing.Process(name=func.__name__, target=func)
        process_list.append(p)
        
        print("Process check : %s, %s" % (p, p.is_alive()))
        p.start()
        time.sleep(0.3)
        
    for p in process_list:
        print("process check : %s %s" % (p, p.is_alive()))
    
    time.sleep(5)
    
    for p in process_list:
        print("Process check : %s %s" % (p, p.is_alive()))
        
        if p.is_alive():
            print("Terminate process : %s" %p)
            p.terminate()
        
    for p in process_list:
        print("Process name: %s, exit code : %s" % (p.name, p.exitcode))
        
if __name__ == "__main__":
    main()
```

> 소스가 길기는 하지만 실행 결과를 보면, `p.exitcode`를 통해서 process의 종료 상태를 확인 할 수가 있음 

``` bash
Process check : <Process(good_job, initial)>, False
Start name: good_job, pid:5805
Process check : <Process(fail_job, initial)>, False
Start name: fail_job, pid:5808
Process check : <Process(kill_job, initial)>, False
Start name: kill_job, pid:5811
process check : <Process(good_job, started)> True
process check : <Process(fail_job, started)> True
process check : <Process(kill_job, started)> True
Exit name:good_job, pid:5805
Exit name:fail_job, pid:5808
Process check : <Process(good_job, stopped)> False
Process check : <Process(fail_job, stopped[1])> False
Process check : <Process(kill_job, started)> True
Terminate process : <Process(kill_job, started)>
Process name: good_job, exit code : 0
Process name: fail_job, exit code : 1
Process name: kill_job, exit code : None
```

### Process Event
> thread 와 마찬가지로 이벤트를 사용할 수 있지만, thread에 있는 기능을 그대로 가고 온 것임.예제는 생략 :) 

### Process Communication

> Process는 event외에 `queue`와 `pipe`를 지원하는데, 먼저 `Queue`를 사용한 예를 보면  

``` python
# process queue

import time
import multiprocessing

def set_data(q):
    p = multiprocessing.current_process()
    msg = "Hello world"
    q.put(msg)
    print("[%s] set queue data : %s" %(p.name, msg))
    
def get_data(q):
    time.sleep(1)
    p = multiprocessing.current_process()
    print("[%s] get queue data : %s" % (p.name, q.get()))

def main():
    queue = multiprocessing.Queue()
    
    p1 = multiprocessing.Process(name="set_data", target=set_data, args=(queue,))
    p1.start()
    
    p2 = multiprocessing.Process(name="get_data", target=get_data, args=(queue,))
    p2.start()
    
    p1.join()
    p2.join()
    
if __name__ == "__main__":
    main()
```
> 결과는 예상한대로, 그 외에도 목적에 따라 `SimpleQueue`, `JoinableQueue` 도 있음.
``` bash
[set_data] set queue data : Hello world
[get_data] get queue data : Hello world
```
  
> `pipe`를 사용한 예제를 보면  

``` python
# process pipe

import multiprocessing

def child(pipe):
    p = multiprocessing.current_process()
    
    msg = "Hello World"
    pipe.send(msg)
    
    print("[%s] send a message to pipe : %s" % (p.name, msg))
    
def main():
    parent_pipe, child_pipe = multiprocessing.Pipe()
    
    p = multiprocessing.Process(name="child", target=child, args=(child_pipe,))
    p.start()
    
    print("Received message : %s" % parent_pipe.recv())
    p.join()
    
if __name__ == "__main__":
    main()
```
실행 결과는 기대했던대로,   
``` bash
[child] send a message to pipe : Hello World
Received message : Hello World
``` 


### Process의 메모리 공유

Thread 나 Process에서는 메모리를 공유하지 않는 것이 좋은데, 어쩔 수 없이 공유할 때가 있음. 이를 위해서 `multiprocessing`에서는 `Value`와 `Array`를 제공하는데, 

``` python
# process shared memory

import multiprocessing

def worker(num, num_list):
    p = multiprocessing.current_process()
    print("[%s] num : %s" % (p.name, num.value))
    
    for idx, value in enumerate(num_list):
        print("[%s] num list[%s] : %s" % (p.name, idx, value))
    
    num.value = 50
    for i in range(len(num_list)):
        num_list[i] = num_list[i]*10
#    num_list = [x*10 for x in num_list]
        
def main():
    single_integer = multiprocessing.Value("i",5)
    integer_list = multiprocessing.Array("i", range(10))
    
    p = multiprocessing.Process(name="worker", target=worker, args=(single_integer, integer_list))
    p.start()
    
    p.join()
    print("num: %s" % (single_integer.value))
    
    for idx, value in enumerate(integer_list):
        print("num list[%s] : %s" % (idx, value))

if __name__ == "__main__":
    main()
```

> `Value`와 `Array`를 사용해서 값을 초기화하는데, 마치 레퍼런스나 포인터를 사용하는 것처럼 값이 변경됨.  
``` bash
[worker] num : 5
[worker] num list[0] : 0
[worker] num list[1] : 1
[worker] num list[2] : 2
[worker] num list[3] : 3
[worker] num list[4] : 4
[worker] num list[5] : 5
[worker] num list[6] : 6
[worker] num list[7] : 7
[worker] num list[8] : 8
[worker] num list[9] : 9
num: 50
num list[0] : 0
num list[1] : 10
num list[2] : 20
num list[3] : 30
num list[4] : 40
num list[5] : 50
num list[6] : 60
num list[7] : 70
num list[8] : 80
num list[9] : 90
```
> 그런데 값을 변경하는 부분을 list comprehension을 사용하면 값이 변경되지 않는데, 내부적으로 무결성을 보장하기 때문임.
` num_list = [x*10 for x in num_list]`

> 이 문제를 해결하기 위해 `manager API`를 사용하는데, 내부적으로 자원을 관리하는 프로세스가 하나 더 있다고 생각하면 됨. 코드를 볼때도 server process는 눈에 보이지 않고 manager가 API를 담당함. 

``` python
# server process

import multiprocessing

def print_array_or_list(name, values):
    for idx, value in enumerate(values):
        print("[%s] num list[%s]: %s" % (name, idx, value))

def worker(v, a, l, d):
    p = multiprocessing.current_process()
    print("[%s] value: %s, dict: %s" % (p.name, v, d["key"]))
    print_array_or_list(p.name, a)
    print_array_or_list(p.name, l)
    
    v.value = 50
    for i in range(len(a)):
        a[i] = a[i] * 10
        
    for i in range(len(l)):
        l[i] = l[i] * 10
        
    d["key"] = "python3"

def main():
    manager = multiprocessing.Manager()
    
    v = manager.Value("i",5)
    a = manager.Array("i", range(10))
    l = manager.list(range(10))
    d = manager.dict()
    d["key"] = "python2"
    
    p = multiprocessing.Process(name="worker", target=worker, args=(v,a,l,d))
    p.start()
    
    p.join()
    main_name="main"
    
    print("[%s] value : %s, dict: %s" % (main_name, v,d["key"]))
    print_array_or_list(main_name, a)
    print_array_or_list(main_name, l)
    
if __name__ == "__main__":
    main()
```
> 실행 결과는 다음과 같음.  

``` bash
[worker] value: Value('i', 5), dict: python2
[worker] num list[0]: 0
[worker] num list[1]: 1
[worker] num list[2]: 2
[worker] num list[3]: 3
[worker] num list[4]: 4
[worker] num list[5]: 5
[worker] num list[6]: 6
[worker] num list[7]: 7
[worker] num list[8]: 8
[worker] num list[9]: 9
[worker] num list[0]: 0
[worker] num list[1]: 1
[worker] num list[2]: 2
[worker] num list[3]: 3
[worker] num list[4]: 4
[worker] num list[5]: 5
[worker] num list[6]: 6
[worker] num list[7]: 7
[worker] num list[8]: 8
[worker] num list[9]: 9
[main] value : Value('i', 50), dict: python3
[main] num list[0]: 0
[main] num list[1]: 10
[main] num list[2]: 20
[main] num list[3]: 30
[main] num list[4]: 40
[main] num list[5]: 50
[main] num list[6]: 60
[main] num list[7]: 70
[main] num list[8]: 80
[main] num list[9]: 90
[main] num list[0]: 0
[main] num list[1]: 10
[main] num list[2]: 20
[main] num list[3]: 30
[main] num list[4]: 40
[main] num list[5]: 50
[main] num list[6]: 60
[main] num list[7]: 70
[main] num list[8]: 80
[main] num list[9]: 90
```
### Process pool

> 프로세스에서 pool을 만들어서 작업을 분리하여 처리할 수 있는데, pool에서 사용할 프로세스의 개수를 입력하면 프로세스의 pool에서 작업과 작업에 필요한 데이터를 나워서 정해진 프로세스 만큼 나눠서 처리.

``` python
# process pool

import multiprocessing

def print_initial_message():
    print("Start process: %s" % multiprocessing.current_process().name)

def worker(data):
    return data*2

def main():
    pool = multiprocessing.Pool(processes=4, initializer=print_initial_message)
    data_list = range(10)
    result = pool.map(worker, data_list)
    
    pool.close()
    pool.join()
    
    print("Result : %s" % result)
    
if __name__ == "__main__":
    main()
```

> pool 을 생성하는데, 이때 사용할 수 있는 프로세스의 개수는 4개로 설정하는데, 즉 4개의 병렬 처리된 작업의 결과가 하나로 합쳐져 출력됨.  
``` bash
Start process: ForkPoolWorker-22
Start process: ForkPoolWorker-24
Start process: ForkPoolWorker-23
Start process: ForkPoolWorker-25
Result : [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
```

### Coroutine

> 협력형 멀티태스킹의 개념인데, 특정 위치에서 실행과 정지를 반복할 수 있도록 함. `Generator`와 비슷한데 제너레이터 `yield`를 통해 값의 반환이 목적이라면 코루틴은 반환과 입력, 둘다 사용할수 있음. 다른 말로 하면 제네레이터는 이터레이터와 같은 역할을 한다면 코루틴은 함수와 같은 역할임  

``` python
# basic coroutine

def coroutine():
    while True:
        msg = yield
        print("Hello, your input message is %s" % msg)

def main():
    c = coroutine()
    next(c)
    next(c)
    c.send("Test")
    c.send("Coroutine")
    
if __name__ == "__main__":
    main()
```
> 결과는 다음과 같은데, 첫번째 출력이 None인 것은 첫번째 'next()`에서 아무런 입력 없이 사용했기 때문임.
``` bash
Hello, your input message is None
Hello, your input message is Test
Hello, your input message is Coroutine
```

> 이제 좀더 복잡한 예제를 보면,

``` python
import time
import random

TOTAL_WORK_LOAD = 50

def worker():
    total_work_load = 0
    worker_name = ""

    while True:
        worker_name, total_work_load = yield (worker_name, total_work_load)

        work_load = random.randrange(1, 10)
        work_load = work_load if total_work_load >= work_load else total_work_load
        total_work_load -= work_load

        print ("[%s] Total : %s, work : %s" % (worker_name, total_work_load, work_load))
        yield total_work_load

def main():
    w1 = worker()
    w2 = worker()

    ret = TOTAL_WORK_LOAD
    while ret > 0:
        next(w1)
        ret = w1.send(("w1", ret))

        next(w2)
        ret = w2.send(("w2", ret))


if __name__ == "__main__":
    main()
```
> 두개의 worker 를 실행 시키면서 적절히 나누어서 처리하고 있음

``` bash
[w1] Total : 45, work : 5
[w2] Total : 41, work : 4
[w1] Total : 36, work : 5
[w2] Total : 33, work : 3
[w1] Total : 28, work : 5
[w2] Total : 25, work : 3
[w1] Total : 19, work : 6
[w2] Total : 11, work : 8
[w1] Total : 2, work : 9
[w2] Total : 0, work : 2 
```
  
  
### Yield From

yield 로 코루틴을 반환하게 되면 코루틴에서 실행된 값을 반환하는게 아니라 코루틴 객체를 반환하게 되고, 다시 `next`를 통해서 값을 가지고 와야 함.

``` python
# return coroutine
def return_one_to_ten():
    for i in range(10):
        yield i

def get_coroutine():
    yield return_one_to_ten()

def main():
    print ("== Get coroutine ==")
    c = get_coroutine()
    print (c)

    print ("== Get coroutine's return value ==")
    ret = next(c)
    print(ret)

    print ("== Get values ==")
    print (next(ret))
    print (list(ret))


if __name__ == "__main__":
    main()
```
> 아래 결과 처럼 get_coroutine은 coroutine을 반환하는되는 것임.

``` bash
== Get coroutine ==
<generator object get_coroutine at 0x7f2ac97dcf10>
== Get coroutine's return value ==
<generator object return_one_to_ten at 0x7f2ac97dc308>
== Get values ==
0
[1, 2, 3, 4, 5, 6, 7, 8, 9]
```

> 그래서 `yield from`을 사용하게 되면

``` python
# yield from

def return_one_to_ten():
    for i in range(10):
        yield i

def get_coroutine():
    yield from return_one_to_ten()

def main():
    print ("== Get coroutine ==")
    c = get_coroutine()
    print (c)

    print ("== Get values ==")
    print (next(c))
    print (list(c))


if __name__ == "__main__":
    main()
```
> get_coroutine을 통해서 제네레이터 객체가 출력되어도 `next`를 바로 사용할 수 있음

``` bash
== Get coroutine ==
<generator object get_coroutine at 0x7f2ac97dcfc0>
== Get values ==
0
[1, 2, 3, 4, 5, 6, 7, 8, 9]
```
update : 2019-07-30. 출처:[파이썬답게 코딩하기](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=143094231)